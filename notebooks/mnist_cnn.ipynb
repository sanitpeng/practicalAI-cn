{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOChJSNXtC9g"
   },
   "source": [
    "# 使用卷积神经网络实现MNIST CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLIxEDq6VhvZ"
   },
   "source": [
    "\n",
    "我的第二个机器学习案例. \n",
    "CNN实现英文手写识别。使用 PyTorch 实现。\n",
    "本代码实现，没有参考mnist的PyTorch实现，主要参考《神经网络与深度学习》版本中:Michael Nielsen的原理实现。\n",
    "该书采用手写代码实现，下文采用PyTorch实现。\n",
    "代码主要参考《madewithml》中文版代码\n",
    "\n",
    "\n",
    "sanit.peng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VoMq0eFRvugb"
   },
   "source": [
    "# 概览 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWro5T5qTJJL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jq65LZJbSpzd"
   },
   "source": [
    "# 训练 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfi_YArvjzrg"
   },
   "source": [
    "*步骤*：$\n",
    "\n",
    "0. mnist数据装载，将图片数据变为（28*28）= 784个输入的数据， 整个数据组织成[N, 784]的一个2维张量和[N]的一个1维张量存放标签。\n",
    "   CNN,要求是一个[N, channel, w, h]的4维张量， 最后在输入模型时候转化成了[N, 1, 28, 28]的张量。\n",
    "   数据加载没有使用torch.dataloader，而是使用在NN（多层感知机)使用的 TensorFlow数据，主要是想把精力集中网络本身，而不用理解数据结构。\n",
    "   同时，考虑将来的一个主要工作就是要将不同的数据转化成需要的数据结构，因此采用更为简单和切合实际的数据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtKqNioAayCy"
   },
   "source": [
    "# 数据 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3OrtMpFayFC"
   },
   "source": [
    "加载mnist数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pvkfS3JZOMgB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/xupeng/opt/anaconda3/lib/python3.8/site-packages (1.7.1)\n",
      "Requirement already satisfied: torchvision in /Users/xupeng/opt/anaconda3/lib/python3.8/site-packages (0.8.2)\n",
      "Requirement already satisfied: numpy in /Users/xupeng/opt/anaconda3/lib/python3.8/site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/xupeng/opt/anaconda3/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /Users/xupeng/opt/anaconda3/lib/python3.8/site-packages (from torchvision) (8.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Load PyTorch library\n",
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for MacOS\n",
    "import os \n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist data, rewriten from mnist_loader\n",
    "import pandas as pd \n",
    "import pickle\n",
    "\n",
    "import gzip\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('../data/mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='bytes')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_d, va_d, te_d = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([5, 0, 4, ..., 8, 4, 8]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_d[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (50000, 784), (50000,))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tr_d[0]),tr_d[0].shape,tr_d[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NfIz_4OPYpG"
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(tr_d[0]).float()\n",
    "y_train = torch.from_numpy(tr_d[1]).long()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_test = torch.from_numpy(te_d[0]).float()\n",
    "y_test = torch.from_numpy(te_d[1]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dim(), y_train.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 784])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 28, 28])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#图像的CNN,需要保留x,y的图像空间结构，将数据重新组装成[N, 28, 28]的张量\n",
    "\n",
    "X_train = X_train.view(X_train.size(0), 28, 28)\n",
    "X_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.view(X_test.size(0), 28, 28)\n",
    "X_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 8, 4, 8])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot one example\n",
    "index = 20\n",
    "plt.imshow(X_train[index].numpy(), cmap='gray')\n",
    "plt.title('%i' % y_train[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "# Arguments\n",
    "args = Namespace(\n",
    "    num_epochs = 2,\n",
    "    num_classes = 10,\n",
    "    batch_size = 100,\n",
    "    learning_rate = 0.001,    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于卷积核的kernel_size, stride, padding, 输入输出的大小：这个文档解释的清楚。https://zhuanlan.zhihu.com/p/77471866\n",
    "简单的描述如下：\n",
    "关于padding： padding的提出是为了解决图片边缘特征被忽略的问题，因此是加在输入的边缘上，比如对于一个，28x28的图案， padding=2, 左边+2， 右边+2，共32x32的图案。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "在卷积时，我们有时候需要卷积前后的尺寸不变。这时候我们就需要用到padding。假设图像的大小，也就是被卷积对象的大小为n*n，卷积核大小为k*k，padding的幅度设为(k-1)/2时，卷积后的输出就为(n-k+2*((k-1)/2))/1+1=n，即卷积输出为n*n，保证了卷积前后尺寸不变。但是如果k是偶数的话，(k-1)/2就不是整数了。\n",
    "\n",
    "本文中，保持卷积层输出为28x28, kernal_size=5， padding=(5-1)/2=2, 卷积输出：（28-5+2*(padding))/1 + 1 = 28\n",
    "\n",
    "关于Stride: 卷积过程中，有时需要通过padding来避免信息损失，有时也要在卷积时通过设置的步长（Stride）来压缩一部分信息，或者使输出的尺寸小于输入的尺寸。简单来讲，Stride的作用：是成倍缩小尺寸，而这个参数的值就是缩小的具体倍数，比如步幅为2，输出就是输入的1/2；步幅为3，输出就是输入的1/3。以此类推。\n",
    "（practicalAI代码注释中，对Padding, Stride的理解有误。sanit.peng）\n",
    "\n",
    "另外参考https://www.jianshu.com/p/f995a9f86aec\n",
    "https://www.freesion.com/article/8111286641/\n",
    "说明n个卷积核，及提取的n特征图的情况\n",
    "\n",
    "特征图的数量理解（个人理解）：\n",
    "nn.Conv2d.out_channels 就是卷积核的个数，因为卷积核的个数=特征图的个数，所以， out_channels=特征图的个数\n",
    "\n",
    "卷积核数目一般都是取2的整数次方，感觉并没有太多trick，具体多少要看效果了。\n",
    "\n",
    "一句话说就是，特征图的数量太少模型拟合能力不够容易欠拟合，反之，特征图数量过多，容易模型拟合能力很强，参数量参加，容易导致过拟合。所以不能太多，也不能太少\n",
    "\n",
    "特征图另一个问题：\n",
    "特征图的大小和输入图的关系：\n",
    "一般来说， 如果特征图的大小减半，那么特征图的个数就要增加1倍，避免丢失太多的信息。\n",
    "在本文中，原代码关于第二层的输入注释是错误的，所以关系无法理解。\n",
    "正确理解：\n",
    "输入层[1，28， 28] --->第一层卷积+池化--->[16, 14, 14],\n",
    "那么第二层，因为经过2x2的池化，特征图输出会变成7x7, 按照上面原则，将特征图数量变为32\n",
    "所以才有第2层参数的设计\n",
    "\n",
    "经过2层后1张28x28的图变成了32张7x7的特征图，虽然有了更多的特征图，但因为特征图对多张输入来说（？？？是否是我理解这样）共享了权重和偏置，所以实际的参数更少，计算效率也更高。\n",
    "\n",
    "\n",
    "特征图的问题：卷积核比如5x5的卷积核，一共有25种，那么当我们取2^4 = 16的时候，取哪16种呢？是否和最后的有关系？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptron \n",
    "class ConvNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNN, self).__init__()\n",
    "\n",
    "        #定义第1层卷积层\n",
    "        #第一层，每个卷积的输入是一个[1, 28, 28]的图形（一张图）， 张量[1, 28, 28]\n",
    "        #输出 [16, 28, 28]图-->池化后--->[16, 14, 14]\n",
    "        self.conv1 = nn.Sequential(         # input shape (1, 28, 28)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              # input height\n",
    "                out_channels=16,            # n_filters\n",
    "                kernel_size=5,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=2,                  # if want same width and length of this image after con2d, padding=(kernel_size-1)/2\n",
    "            ),                              # output shape (16, 28, 28)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area, output shape (16, 14, 14)\n",
    "        )\n",
    "        \n",
    "        #定义第2层卷积层\n",
    "        #每个卷积的输入是一个[16, 14, 14]的特征图（16张图）\n",
    "        #输出 [32, 7, 7]图-->池化后--->[32, 7, 7]， 32张特征图\n",
    "        \n",
    "        self.conv2 = nn.Sequential(         # input shape (16, 14, 14)\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,              # input height\n",
    "                out_channels=32,            # n_filters\n",
    "                kernel_size=5,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=2,                  # if want same width and length of this image after con2d, padding=(kernel_size-1)/2\n",
    "            ),                              # output shape (32, 14, 14)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area, output shape (32, 7, 7)\n",
    "        )\n",
    "\n",
    "        #32x7x7的全连接\n",
    "        self.out = nn.Linear(32 * 7 * 7, args.num_classes)   # fully connected layer, output 10 classes\n",
    "        \n",
    "\n",
    "    def forward(self, x):        \n",
    "        x = self.conv1(x)\n",
    "        _features = x = self.conv2(x)\n",
    "        #所以，这里的x应该是 [N, 32, 7, 7]的一个张量， 变成一个全连接的输入， [N, 32*7*7]\n",
    "        #print(x.size())\n",
    "        x = x.view(x.size(0), -1)           # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        output = self.out(x)\n",
    "        return output, x, _features    # return x for visualization        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "\n",
    "model = ConvNN()\n",
    "print(model)  # net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization,\n",
    "#损失函数 和 学习路径\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate) # Adam optimizer (usually better than SGD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#损失函数 和 学习路径SGD\n",
    "#optimizer = optim.SGD(model.parameters(), lr=args.learning_rate) # SGD optimizer (usually better than SGD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 1, 28, 28]), torch.Size([50000]))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#由于神经网络要求图片四维——数量×通道数×长×宽\n",
    "#需要扩展维度\n",
    "\n",
    "\n",
    "\n",
    "X_CNN_train = X_train.unsqueeze(1)\n",
    "X_CNN_train.size(), y_train.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy\n",
    "def get_accuracy(y_pred, y_target):\n",
    "    n_correct = torch.eq(y_pred, y_target).sum().item()\n",
    "    accuracy = n_correct / len(y_pred) * 100\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/500], Loss: 0.1720, acc:96.00%\n",
      "Epoch [1/2], Step [200/500], Loss: 0.1277, acc:95.00%\n",
      "Epoch [1/2], Step [300/500], Loss: 0.2097, acc:92.00%\n",
      "Epoch [1/2], Step [400/500], Loss: 0.1191, acc:95.00%\n",
      "Epoch [1/2], Step [500/500], Loss: 0.1353, acc:94.00%\n",
      "torch.Size([100, 32, 7, 7])\n",
      "Epoch [2/2], Step [100/500], Loss: 0.0310, acc:99.00%\n",
      "Epoch [2/2], Step [200/500], Loss: 0.0277, acc:100.00%\n",
      "Epoch [2/2], Step [300/500], Loss: 0.0857, acc:95.00%\n",
      "Epoch [2/2], Step [400/500], Loss: 0.0640, acc:99.00%\n",
      "Epoch [2/2], Step [500/500], Loss: 0.0784, acc:97.00%\n",
      "torch.Size([100, 32, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "batch_num = int(X_CNN_train.size(0) / args.batch_size)\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    for i in range(batch_num):\n",
    "        index_from = i * args.batch_size\n",
    "        index_to = (i + 1) * args.batch_size\n",
    "        \n",
    "        images = X_CNN_train[index_from:index_to, ]\n",
    "        labels = y_train[index_from:index_to, ]\n",
    "        # Forward pass\n",
    "        y_pred, _, features = model(images)\n",
    "        \n",
    "        # Accuracy\n",
    "        _, predictions = y_pred.max(dim=1)\n",
    "        accuracy = get_accuracy(y_pred=predictions.long(), y_target=labels)        \n",
    "        \n",
    "        loss = loss_fn(y_pred, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, acc:{:.2f}%' \n",
    "                   .format(epoch+1, args.num_epochs, i+1, batch_num, loss.item(), accuracy))\n",
    "    print(features.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 32, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "print(features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 1, 28, 28]), torch.Size([10000]))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_CNN_test = X_test.unsqueeze(1)\n",
    "X_CNN_test.size(), y_test.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNN(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 98.29 %\n"
     ]
    }
   ],
   "source": [
    "#不在产生梯度，不影响网络权重\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    length = int(X_CNN_test.size(0) / args.batch_size)\n",
    "    for i in range(length):\n",
    "                \n",
    "        index_from = i * args.batch_size\n",
    "        index_to = (i + 1) * args.batch_size\n",
    "                \n",
    "        images = X_CNN_test[index_from:index_to, ]\n",
    "        labels = y_test[index_from:index_to, ]\n",
    "        # Forward pass\n",
    "        y_pred, _= model(images)\n",
    "        \n",
    "        \n",
    "\n",
    "        _, predictions = y_pred.max(dim=1)\n",
    "        #or \n",
    "        #_, predictions = torch.max(y_pred.data, 1)\n",
    "        accuracy = get_accuracy(y_pred=predictions.long(), y_target=labels)\n",
    "        \n",
    "        #print(\"The \", i, \" acc=\", accuracy)\n",
    "        correct += accuracy     \n",
    "                \n",
    "        \n",
    "    print('Test Accuracy of the model on the test images: {} %'.format(correct / length))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'mnist_cnn_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAANAUlEQVR4nO3db4gc9R3H8c8nmjxJqiRNjMHGphaRlmJNDVKwqKW0qAixaEuDSEolF6GRCj6opGBiqiC1f+gDjV5RmpZqKUQxBmkqoTTtk5I7STU2bc5KGtM7koqIig9avW8f3KScye3suTOzs7nv+wXH7s53d+abyX1uZnZ29ueIEIC5b17bDQDoD8IOJEHYgSQIO5AEYQeSOLufC7PNW/9AwyLCM02vtGW3fa3tv9t+xfbdVeYFoFnu9Ty77bMkHZb0ZUnHJO2XtC4i/lryGrbsQMOa2LJfIemViHg1Iv4j6deS1laYH4AGVQn7BZJem/b4WDHtA2wP2R6xPVJhWQAqqvIG3Uy7CqftpkfEsKRhid14oE1VtuzHJK2c9vhjksartQOgKVXCvl/SxbY/YXuBpG9I2lVPWwDq1vNufES8Z3uTpD2SzpL0eES8XFtnAGrV86m3nhbGMTvQuEY+VAPgzEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEj0P2Ywc1qxZU1rfv39/aX1ycrLOdj5gy5YtpfX77ruvsWWfiSqF3fYRSW9Lel/SexFR/psBoDV1bNm/GBGv1zAfAA3imB1IomrYQ9LvbI/aHprpCbaHbI/YHqm4LAAVVN2NvzIixm2fJ+l523+LiH3TnxARw5KGJcl2VFwegB5V2rJHxHhxe0LS05KuqKMpAPXrOey2F9r+yMn7kr4i6WBdjQGoV5Xd+OWSnrZ9cj5PRMRva+kKA6Pbuexu59HbPM++bNmyjrWdO3eWvnbfvn2l9TNRz2GPiFclfbbGXgA0iFNvQBKEHUiCsANJEHYgCcIOJMElrnPcqlWrSut79uwprZ9//vk1dtNfmzZt6lg7fPhw6Wvn4qk3tuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATn2ee4s88u/y++6KKL+tQJ2saWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4Dz7HLdt27a2W+how4YNpfXLL7+8tH777bfX2c6cx5YdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5LgPPsZ4Lrrriut7969u0+dnO7+++8vrd9zzz09z/ucc84prc+bV76tKqsXQ42n0nXLbvtx2ydsH5w2bYnt522PFbeLm20TQFWz2Y3/uaRrT5l2t6S9EXGxpL3FYwADrGvYI2KfpDdOmbxW0o7i/g5JN9bbFoC69XrMvjwiJiQpIiZsn9fpibaHJA31uBwANWn8DbqIGJY0LEm2o+nlAZhZr6fejtteIUnF7Yn6WgLQhF7DvkvS+uL+eknP1NMOgKZ03Y23/aSkayQttX1M0hZJD0j6je3bJB2V9LUmm0S5ycnJ1pZd5Tx6NxHlR31V/t3d5j0XdQ17RKzrUPpSzb0AaBAflwWSIOxAEoQdSIKwA0kQdiAJLnE9A9x7772tLXt8fLyxeS9YsKC0vnTp0saWnRFbdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgvPsZ4CRkZHS+urVqxtb9tBQc98odscdd5TWN2/e3NiyM2LLDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJcJ79DLBx48bSepWvVN61a1dpfXR0tOd5d9PkOXycji07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBefYB8Nxzz5XW583r/W/y2NhYaf2mm27qed5V2S6tV/l3S9KePXs61h566KFK8z4TdV2bth+3fcL2wWnTttr+l+0Dxc/1zbYJoKrZ/On8uaRrZ5j+k4i4rPgp3zQBaF3XsEfEPklv9KEXAA2qclC0yfaLxW7+4k5Psj1ke8R2+RepAWhUr2HfLumTki6TNCHpR52eGBHDEbEmItb0uCwANegp7BFxPCLej4hJST+TdEW9bQGoW09ht71i2sOvSjrY6bkABkPX8+y2n5R0jaSlto9J2iLpGtuXSQpJRySVX3Cd3NVXX11av+SSS0rr3a5XL6tHROlrm3bzzTd3rC1ZsqT0tVWu05ek7du3V3r9XNM17BGxbobJjzXQC4AG8XFZIAnCDiRB2IEkCDuQBGEHkuAS1z649NJLS+sXXnhhnzqp38KFC0vrN9xwQ8faueeeW2nZGzZsKK0/++yzleY/17BlB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkOM8+x3UbkrmqBx98sLR+yy23NLbsiYmJxuY9F7FlB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkOM8+xw0PD1d6/bZt20rrGzeWf4t4la+D7vYZgdHR0Z7nnRFbdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgvPsfWC7tD5vXvnf3G71MldddVVp/a677iqtdzuPXqW3J554orR+66239jxvnK7r/5TtlbZ/b/uQ7Zdtf6eYvsT287bHitvFzbcLoFez+bP8nqS7IuJTkj4v6du2Py3pbkl7I+JiSXuLxwAGVNewR8RERLxQ3H9b0iFJF0haK2lH8bQdkm5sqEcANfhQx+y2V0laLenPkpZHxIQ09QfB9nkdXjMkaahinwAqmnXYbS+StFPSnRHxVrc3nU6KiGFJw8U8opcmAVQ3q7dSbc/XVNB/FRFPFZOP215R1FdIOtFMiwDq0HXL7qlN+GOSDkXEj6eVdklaL+mB4vaZRjqcAyLKd2iqXAba7fVVL3FtsretW7dWmjc+nNnsxl8p6VZJL9k+UEzbrKmQ/8b2bZKOSvpaIx0CqEXXsEfEnyR1OkD/Ur3tAGgKH5cFkiDsQBKEHUiCsANJEHYgCS5x7YM333yztP7uu++W1hctWlRjN/UaGxsrrT/yyCMda0ePHq27HZRgyw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSbjbtda1LoxvqplRt69rfvjhh0vrVa85r2L+/PmtLRszi4gZr1Jlyw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSXA9+wB49NFHS+vLli0rrW/ZsqVjbXx8vPS1Q0OMzJUFW3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLr9ey2V0r6haTzJU1KGo6In9reKmmDpH8XT90cEc91mRfXswMN63Q9+2zCvkLSioh4wfZHJI1KulHS1yW9ExE/nG0ThB1oXqewz2Z89glJE8X9t20fknRBve0BaNqHOma3vUrSakl/LiZtsv2i7cdtL+7wmiHbI7ZHqrUKoIpZfwed7UWS/iDp/oh4yvZySa9LCknf19Su/re6zIPdeKBhPR+zS5Lt+ZJ2S9oTET+eob5K0u6I+EyX+RB2oGE9f+GkbUt6TNKh6UEv3rg76auSDlZtEkBzZvNu/Bck/VHSS5o69SZJmyWtk3SZpnbjj0jaWLyZVzYvtuxAwyrtxteFsAPN43vjgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSfR7yObXJf1z2uOlxbRBNKi9DWpfEr31qs7ePt6p0Nfr2U9buD0SEWtaa6DEoPY2qH1J9NarfvXGbjyQBGEHkmg77MMtL7/MoPY2qH1J9NarvvTW6jE7gP5pe8sOoE8IO5BEK2G3fa3tv9t+xfbdbfTQie0jtl+yfaDt8emKMfRO2D44bdoS28/bHituZxxjr6Xettr+V7HuDti+vqXeVtr+ve1Dtl+2/Z1ieqvrrqSvvqy3vh+z2z5L0mFJX5Z0TNJ+Sesi4q99baQD20ckrYmI1j+AYfsqSe9I+sXJobVs/0DSGxHxQPGHcnFEfHdAetuqDzmMd0O9dRpm/Jtqcd3VOfx5L9rYsl8h6ZWIeDUi/iPp15LWttDHwIuIfZLeOGXyWkk7ivs7NPXL0ncdehsIETERES8U99+WdHKY8VbXXUlffdFG2C+Q9Nq0x8c0WOO9h6Tf2R61PdR2MzNYfnKYreL2vJb7OVXXYbz76ZRhxgdm3fUy/HlVbYR9pqFpBun835UR8TlJ10n6drG7itnZLumTmhoDcELSj9psphhmfKekOyPirTZ7mW6Gvvqy3toI+zFJK6c9/pik8Rb6mFFEjBe3JyQ9ranDjkFy/OQIusXtiZb7+b+IOB4R70fEpKSfqcV1VwwzvlPSryLiqWJy6+tupr76td7aCPt+SRfb/oTtBZK+IWlXC32cxvbC4o0T2V4o6SsavKGod0laX9xfL+mZFnv5gEEZxrvTMONqed21Pvx5RPT9R9L1mnpH/h+SvtdGDx36ukjSX4qfl9vuTdKTmtqt+6+m9ohuk/RRSXsljRW3Swaot19qamjvFzUVrBUt9fYFTR0avijpQPFzfdvrrqSvvqw3Pi4LJMEn6IAkCDuQBGEHkiDsQBKEHUiCsANJEHYgif8B2Wr6jAuSxEIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#单个测试，绘图\n",
    "index = 34\n",
    "image = X_train[index]\n",
    "plt.imshow(image.numpy(), cmap='gray')\n",
    "image = image.unsqueeze(0)\n",
    "image = image.unsqueeze(0)\n",
    "image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, _= model(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the picture is  0 possibility is  8.540617942810059\n"
     ]
    }
   ],
   "source": [
    "y_prob, predictions = y_pred.max(dim=1)\n",
    "print(\"the picture is \", predictions.item(), \"possibility is \", y_prob.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 7])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature1 = features[0][19]\n",
    "feature1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.8465, 1.3268, 1.1874, 0.2436, 0.0000],\n",
       "        [0.0079, 0.1972, 0.4685, 0.0000, 0.0000, 0.1430, 0.1279],\n",
       "        [0.0079, 0.2335, 0.0000, 2.3665, 4.1026, 3.2674, 0.5590],\n",
       "        [0.0079, 0.0000, 0.0000, 1.4998, 3.1068, 3.0638, 1.0877],\n",
       "        [0.0079, 0.0000, 0.0000, 0.7868, 1.2675, 0.2985, 0.0615],\n",
       "        [0.0079, 0.0000, 0.0000, 0.0000, 0.0000, 0.3705, 0.3223],\n",
       "        [0.0505, 0.0812, 0.5991, 2.1404, 2.4730, 1.5793, 0.2403]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f94974b78b0>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(feature1.detach().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08_Multilayer_Perceptron-cn",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
